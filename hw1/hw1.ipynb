{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import scipy.stats as s\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N is 1, probability is 0.382924922548\n",
      "N is 10, probability is 0.886153701993\n",
      "N is 20, probability is 0.974652681323\n"
     ]
    }
   ],
   "source": [
    "for N in (1,10,20):\n",
    "    z = s.norm(loc=0, scale=1)\n",
    "    mu = 10\n",
    "    sigma = 1.0/math.sqrt(N)\n",
    "    a = (10.5-mu)/sigma\n",
    "    b = (9.5-mu)/sigma\n",
    "    print(\"N is {}, probability is {}\".format(N, z.cdf(a) - z.cdf(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N is 1, lower bound is 0\n",
      "N is 10, lower bound is 0.42699040628\n",
      "N is 20, lower bound is 0.835830002752\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "for N in (1,10,20):\n",
    "    sigma = 1.0\n",
    "    bound = max([0, 1 - 2*math.exp(-N*(epsilon**2)/(2*sigma**2))])\n",
    "    print(\"N is {}, lower bound is {}\".format(N, bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True CDF was in limits on 97.4% of the time\n"
     ]
    }
   ],
   "source": [
    "std_norm = s.norm(loc=0, scale=1)\n",
    "N = 100\n",
    "alpha = 0.05\n",
    "x_values = np.linspace(-6, 6, num=1000)\n",
    "\n",
    "counter = 0\n",
    "rounds = 1000\n",
    "true_cdf = s.norm.cdf(x_values)\n",
    "worst_error = -1\n",
    "best_error = np.inf\n",
    "for i in range(rounds):\n",
    "    # Sample and define empirical cdf + CI\n",
    "    random_samples = std_norm.rvs(size=N)\n",
    "    empirical_cdf = lambda x: np.sum(random_samples.reshape(1, N)<x.reshape(x.size,1), axis=1)/N\n",
    "    epsilon = np.sqrt(np.log(2/alpha)/(2*N))\n",
    "    high_limit = lambda x: empirical_cdf(x) + epsilon\n",
    "    low_limit = lambda x: empirical_cdf(x) - epsilon\n",
    "\n",
    "    # Find best and worst cdf's\n",
    "    error = np.max(np.abs(true_cdf - empirical_cdf(x_values)))\n",
    "    if error > worst_error:\n",
    "        worst_error = error\n",
    "        worst_experience = empirical_cdf(x_values)\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_experience = empirical_cdf(x_values)\n",
    "\n",
    "    # Check if true cdf is in the CI\n",
    "    is_below_high_limit = np.all(true_cdf < high_limit(x_values))\n",
    "    is_above_low_limit = np.all(true_cdf > low_limit(x_values))\n",
    "    is_in_limits = is_below_high_limit and is_above_low_limit\n",
    "    counter += is_in_limits\n",
    "print(\"True CDF was in limits on {}% of the time\".format(100*counter/rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true CDF and best and worst empirical CDFs\n",
    "plt.figure()\n",
    "plt.plot(x_values, worst_experience, label='worst')\n",
    "plt.plot(x_values, best_experience, label='best')\n",
    "plt.plot(x_values, true_cdf, label='true')\n",
    "plt.legend()\n",
    "plt.title('Best, worst and true CDFs')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "samsungData = pd.read_csv('samsungData.csv').drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_correlation = samsungData.corr().abs()\n",
    "plt.figure()\n",
    "plt.imshow(pair_correlation)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Feature number\")\n",
    "plt.ylabel(\"Feature number\")\n",
    "plt.title(\"Pairwise correlation of features\")\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tBodyAccMag-mean() tBodyAccMag-sma()\n",
      "tBodyAccMag-mean() tGravityAccMag-mean()\n",
      "tBodyAccMag-mean() tGravityAccMag-sma()\n",
      "tBodyAccMag-std() tGravityAccMag-std()\n",
      "tBodyAccMag-mad() tGravityAccMag-mad()\n",
      "tBodyAccMag-max() tGravityAccMag-max()\n",
      "tBodyAccMag-min() tGravityAccMag-min()\n",
      "tBodyAccMag-sma() tBodyAccMag-mean()\n",
      "tBodyAccMag-sma() tGravityAccMag-mean()\n",
      "tBodyAccMag-sma() tGravityAccMag-sma()\n",
      "tBodyAccMag-energy() tGravityAccMag-energy()\n",
      "tBodyAccMag-iqr() tGravityAccMag-iqr()\n",
      "tBodyAccMag-entropy() tGravityAccMag-entropy()\n",
      "tBodyAccMag-arCoeff()1 tGravityAccMag-arCoeff()1\n",
      "tBodyAccMag-arCoeff()2 tGravityAccMag-arCoeff()2\n",
      "tBodyAccMag-arCoeff()3 tGravityAccMag-arCoeff()3\n",
      "tBodyAccMag-arCoeff()4 tGravityAccMag-arCoeff()4\n",
      "tGravityAccMag-mean() tBodyAccMag-mean()\n",
      "tGravityAccMag-mean() tBodyAccMag-sma()\n",
      "tGravityAccMag-mean() tGravityAccMag-sma()\n",
      "tGravityAccMag-std() tBodyAccMag-std()\n",
      "tGravityAccMag-mad() tBodyAccMag-mad()\n",
      "tGravityAccMag-max() tBodyAccMag-max()\n",
      "tGravityAccMag-min() tBodyAccMag-min()\n",
      "tGravityAccMag-sma() tBodyAccMag-mean()\n",
      "tGravityAccMag-sma() tBodyAccMag-sma()\n",
      "tGravityAccMag-sma() tGravityAccMag-mean()\n",
      "tGravityAccMag-energy() tBodyAccMag-energy()\n",
      "tGravityAccMag-iqr() tBodyAccMag-iqr()\n",
      "tGravityAccMag-entropy() tBodyAccMag-entropy()\n",
      "tGravityAccMag-arCoeff()1 tBodyAccMag-arCoeff()1\n",
      "tGravityAccMag-arCoeff()2 tBodyAccMag-arCoeff()2\n",
      "tGravityAccMag-arCoeff()3 tBodyAccMag-arCoeff()3\n",
      "tGravityAccMag-arCoeff()4 tBodyAccMag-arCoeff()4\n",
      "tBodyAccJerkMag-mean() tBodyAccJerkMag-sma()\n",
      "tBodyAccJerkMag-sma() tBodyAccJerkMag-mean()\n",
      "tBodyGyroMag-mean() tBodyGyroMag-sma()\n",
      "tBodyGyroMag-sma() tBodyGyroMag-mean()\n",
      "tBodyGyroJerkMag-mean() tBodyGyroJerkMag-sma()\n",
      "tBodyGyroJerkMag-sma() tBodyGyroJerkMag-mean()\n",
      "fBodyAccMag-mean() fBodyAccMag-sma()\n",
      "fBodyAccMag-sma() fBodyAccMag-mean()\n",
      "fBodyBodyAccJerkMag-mean() fBodyBodyAccJerkMag-sma()\n",
      "fBodyBodyAccJerkMag-sma() fBodyBodyAccJerkMag-mean()\n",
      "fBodyBodyGyroMag-mean() fBodyBodyGyroMag-sma()\n",
      "fBodyBodyGyroMag-sma() fBodyBodyGyroMag-mean()\n",
      "fBodyBodyGyroJerkMag-mean() fBodyBodyGyroJerkMag-sma()\n",
      "fBodyBodyGyroJerkMag-sma() fBodyBodyGyroJerkMag-mean()\n",
      "\n",
      "We have 48 features pairs with maximum correlation (=1)\n"
     ]
    }
   ],
   "source": [
    "x_features, y_features = np.where(np.array(pair_correlation) == 1)\n",
    "counter = 0\n",
    "for x, y in zip(x_features, y_features):\n",
    "    # Feature have a strong correlation to themselves, so skip displaying them\n",
    "    if x == y:\n",
    "        continue\n",
    "    print(samsungData.keys()[x], samsungData.keys()[y],)\n",
    "    counter +=1 \n",
    "    \n",
    "print(\"\\nWe have %d features pairs with maximum correlation (=1)\" % counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between tGravityAccMag-sma and tGravityAccMag-mean: 0.000000\n",
      "Max difference between tGravityAccMag-sma and tGravityAccMag-mean: 0.000000\n"
     ]
    }
   ],
   "source": [
    "diff_1 = samsungData['tGravityAccMag-sma()'] - samsungData['tGravityAccMag-mean()']\n",
    "diff_2 = samsungData['tBodyAccJerkMag-mean()'] - samsungData['tBodyAccJerkMag-sma()']\n",
    "print(\"Max difference between tGravityAccMag-sma and tGravityAccMag-mean: %f\" % np.max(np.abs(diff_1)))\n",
    "print(\"Max difference between tGravityAccMag-sma and tGravityAccMag-mean: %f\" % np.max(np.abs(diff_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some features are identical and therefore redundant\n"
     ]
    }
   ],
   "source": [
    "print(\"Some features are identical and therefore redundant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(samsungData['activity'])\n",
    "class_correlation = {}\n",
    "for i, cls in enumerate(classes):\n",
    "    class_data = samsungData[samsungData['activity'] == cls]\n",
    "    correlation = np.abs(class_data.corr())\n",
    "    class_correlation[cls] = correlation\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    plt.title(cls)\n",
    "    plt.imshow(correlation)\n",
    "    plt.colorbar()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6\n",
    "\n",
    "The same features pairs with maximum correleation 1, still have the same correleation 1 per classes that contain them. Still many features are redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = samsungData.shape[0]\n",
    "\n",
    "def bootstrap_correlation_std(ind1, ind2, iterations):\n",
    "    feature_1 = np.array(samsungData[samsungData.columns[ind1]])\n",
    "    feature_2 = np.array(samsungData[samsungData.columns[ind2]])\n",
    "    correlations = np.zeros([iterations])\n",
    "    correlations_std = np.zeros([iterations])\n",
    "    correlations_sum = 0\n",
    "    correlations_means = np.zeros([iterations])\n",
    "    for i in range(iterations):\n",
    "        rand_indexes = np.random.randint(0, N, [N])\n",
    "        iter_features_1 = feature_1[rand_indexes]\n",
    "        iter_features_2 = feature_2[rand_indexes]\n",
    "        iter_features_1 = (iter_features_1-np.mean(iter_features_1))/np.std(iter_features_1)\n",
    "        iter_features_2 = (iter_features_2-np.mean(iter_features_2))/np.std(iter_features_2)\n",
    "        correlations[i] = np.mean(np.multiply(iter_features_1, iter_features_2))\n",
    "        correlation = np.mean(np.multiply(iter_features_1, iter_features_2))\n",
    "        correlations_sum += correlation\n",
    "        correleations_mean = correlations_sum / (i+1)\n",
    "        correlations_means[i] = correleations_mean\n",
    "        correlations_std[i] = np.sqrt(np.mean((correlations[:i+1] - correleations_mean)**2))\n",
    "    return correlations_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1300\n",
    "pairs = [[42, 54],\n",
    "        [335, 365],\n",
    "        [89, 454]]\n",
    "correlations_std = []\n",
    "\n",
    "for [ind1, ind2] in pairs:\n",
    "    correlations_std_iterations = bootstrap_correlation_std(ind1, ind2, iterations)\n",
    "    correlations_std.append(correlations_std_iterations[-1])\n",
    "    plt.figure()\n",
    "    plt.plot(correlations_std_iterations)\n",
    "    plt.xlabel('num of iterations')\n",
    "    plt.ylabel('bootstrap estimator for correlation variance')\n",
    "    plt.title(\"Correlation variance for {} and {}\".format(samsungData.columns[ind1], samsungData.columns[ind2]))\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define $c = Cor(A, B)$, the correlation between columns A and B; and $\\hat{c} = Cor_n(A,B)$ the empirical-correlation between A and B that we calculated in previous sections. $\\hat{c}$ is unbiased. Proof (assuming A and B are normalized to have $E=0, Var=1$):\n",
    "$$E(\\hat c) = E \\big[ E_n(A\\cdot B)\\big] \\underbrace{=}_{\\text{$E_n$ and $E$ are linear}} E_n[ E(A\\cdot B)] = E(A\\cdot B) = Cor(A,B) = c$$\n",
    "\n",
    "Thereofore, to get a CI of 95% we can use Chebyshev's inequality:\n",
    "\n",
    "$$ P(| \\hat c - c | > \\epsilon) \\le \\frac{\\sigma^2_{\\hat c}}{\\epsilon} = 1-0.95=0.05 $$\n",
    "\n",
    "Now we can substitute $\\sigma$ with the estimated $\\hat \\sigma$ and get:\n",
    "\n",
    "$$ \\epsilon \\approx \\frac{\\hat{\\sigma}^2_{\\hat c}}{0.05} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 95% CI of correlation for columns [42, 54] is 0.9964±0.0077\n",
      "A 95% CI of correlation for columns [335, 365] is 0.8077±0.1709\n",
      "A 95% CI of correlation for columns [89, 454] is 0.0310±0.2211\n"
     ]
    }
   ],
   "source": [
    "for i, pair in enumerate(pairs):\n",
    "    epsilon = correlations_std[i]/0.05\n",
    "    ind1, ind2 = pair\n",
    "    col1 = samsungData.columns[ind1]\n",
    "    col2 = samsungData.columns[ind2]\n",
    "    correlation = pair_correlation[col1][col2]\n",
    "    print(\"A 95% CI of correlation for columns {0} is {1:.4f}±{2:.4f}\".format(pair, correlation, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all graph windows\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
